{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aim: Find and Download any coincident ICESat-2 tracks and Sentinel-1/2 imagery within a given geographic area and date range. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                 #DataFrames\n",
    "import geopandas as gpd             #GeoDataFrames\n",
    "import icepyx as ipx                #Connect to Icesat-2 API\n",
    "from sentinelsat import SentinelAPI #Connect to Sentinel API\n",
    "import os                           #General Utilities \n",
    "import shapely.wkt                  #Manipulate geometry\n",
    "from shapely.geometry import Point  #Points \n",
    "from zipfile import ZipFile         #Unzip files\n",
    "import readers as rd                #Convert H5 to Dataframe\n",
    "import h5py                         #Read H5 \n",
    "from datetime import datetime, date, timedelta "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IS2 Product type (ATL03:Photon Heights);(ATL06:Land Ice Height);(ATL07:Sea Ice Height);(ATL10:Freeboard)\n",
    "p_name = 'ATL10'   \n",
    "\n",
    "#Sentinel-1 or Sentinel-2\n",
    "platform = 'Sentinel-1'\n",
    "\n",
    "#Maximum cloud cover (For Sentinel-2 Only)\n",
    "max_cloud = 30\n",
    "\n",
    "#Date Range as (YYYY,M,D)\n",
    "date_1 = date(2020,6,1) \n",
    "date_2 = date(2020,10,1)\n",
    "\n",
    "#Search Footprint for Sentinel images can be any WKT geometry. \n",
    "#https://arthur-e.github.io/Wicket/sandbox-gmaps3.html\n",
    "footprint = 'POINT(-48.78025092888943 66.77289756577241)'\n",
    "\n",
    "#(Optional) Search footprint for is2 track. Useful when looking at specific areas. If blank will use above footprint instead\n",
    "is2_footprint = 'POLYGON((-49.28562202263943 68.16996339879951,-48.31882514763943 68.16996339879951,-48.31882514763943 65.28249233286489,-49.28562202263943 65.28249233286489,-49.28562202263943 68.16996339879951))'\n",
    "\n",
    "#Connect to Copernicus API for Sentinel\n",
    "global api\n",
    "api = SentinelAPI('username', 'password', 'https://scihub.copernicus.eu/dhus/')\n",
    "\n",
    "#Enter user details to connect to Icesat-2 API\n",
    "earthdata_uid = 'username'\n",
    "email = 'email'\n",
    "\n",
    "#Where to output folders\n",
    "out_path = 'E:/Cryo2ice/pycesat/outputs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Sentinel Images\n",
    "\n",
    "These Functions use user inputs to retrieve a list of Sentinel-2 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_S2_images(footprint, date_1, date_2, api, max_cloud):\n",
    "    #Get all products from user imputs\n",
    "    products = api.query(footprint, \n",
    "                         date=(date_1, date_2), \n",
    "                         platformname = 'Sentinel-2', \n",
    "                         cloudcoverpercentage=(0, max_cloud),\n",
    "                         area_relation='Contains', #Can be changed \n",
    "                         producttype='S2MSI1C') \n",
    "    \n",
    "    df = api.to_geodataframe(products)\n",
    "    os.remove('product_list.csv') #Comment out if needed\n",
    "    df.to_csv('product_list.csv', sep=',')   \n",
    "    global pid_list\n",
    "    pid_list = df.index\n",
    "    return pid_list\n",
    "\n",
    "\n",
    "def find_S1_images(footprint, date_1, date_2, api):\n",
    "    #Get all products from user imputs\n",
    "    products = api.query(footprint, \n",
    "                         date=(date_1, date_2), \n",
    "                         platformname = 'Sentinel-1', \n",
    "                         sensoroperationalmode = 'EW', #EW is low Res, IW high res\n",
    "                         producttype='GRD', #GRD gridded geocoded, SLC for \n",
    "                         area_relation='Contains')\n",
    "    \n",
    "    df = api.to_geodataframe(products) #Comment out if needed\n",
    "    os.remove('product_list.csv')\n",
    "    df.to_csv('product_list.csv', sep=',')    \n",
    "    global pid_list\n",
    "    pid_list = df.index\n",
    "    return pid_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Coincident ICESat-2 tracks\n",
    "\n",
    "The following function is run for each Sentinel image in pid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_IS_tracks(pid, p_name, df, api, footprint='Sentinel'):\n",
    "    #If is_footprint was not input by user, search the entire footprint of the Sentinel Image\n",
    "    if footprint == 'Sentinel':  \n",
    "        footprint = api.get_product_odata(pid).get('footprint')\n",
    "        \n",
    "    polygon = shapely.wkt.loads(footprint)\n",
    "    polygon = str(polygon.bounds)[1:-1]\n",
    "    polygon = polygon.split(sep=',')\n",
    "    polygon = [float(i) for i in polygon]\n",
    "    \n",
    "    s1_date = api.get_product_odata(pid, full = True).get('Sensing start')\n",
    "    s1_name = api.get_product_odata(pid, full = True).get('title')\n",
    "    \n",
    "    #Calculate time window around Sentinel Image\n",
    "    #Note: Icepyx does not currently support time periods shorter than a day, therefore hours=1 does nothing here....\n",
    "    #... The date_range used is one day. Unfortunately this means some images near Midnight might not be used. \n",
    "    min_time = (s1_date - timedelta(hours=1)) \n",
    "    max_time = (s1_date + timedelta(hours=1))\n",
    "    date_range = [str(s1_date - timedelta(hours=1))[:10], str(s1_date + timedelta(hours=1))[:10]]\n",
    "    \n",
    "    #Run API Query & store products in region_a\n",
    "    region_a = ipx.Query(p_name, polygon, date_range)\n",
    "    \n",
    "    #Print How many tracks pass through the image on the same day\n",
    "    print(len(region_a.granules.avail))\n",
    "    \n",
    "    #Pretty sure this was for bug testing but I am afraid to remove it. (Check back later)\n",
    "    global co_track_list\n",
    "    co_track_list = [] \n",
    "    global other_tracks\n",
    "    other_tracks = []\n",
    "    \n",
    "    #For each image Calculate exact time interval between Sentinel image and ICESat-2 track\n",
    "    for dic in region_a.granules.avail:\n",
    "        item = dic['time_start']\n",
    "        is_time = datetime.datetime.strptime(item, '%Y-%m-%dT%H:%M:%S.%fZ') #Extract datetime from string\n",
    "        is_time_dif = abs(is_time - s1_date)\n",
    "        if min_time < is_time < max_time:\n",
    "            co_track_list.append(is_time_dif)\n",
    "        else:\n",
    "            other_tracks.append(is_time_dif)\n",
    "        #Append Coincident tracks and images to the end of the Dataframe\n",
    "        df.loc[len(df)] = [dic['producer_granule_id'], is_time, s1_name, s1_date, is_time_dif, pid, polygon]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the ICESat-2 Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_is2_data(time_gap, df, p_name, directory_path, earthdata_uid, email):\n",
    "    #Check that the User wants to download the products\n",
    "    input('Press Enter to order & download the ICESat-2 products') \n",
    "    n = 0\n",
    "    #for each track in dataframe, download if the time gap is lower than input\n",
    "    #Note: If the download fails, the Sentinel image will still be downloaded and vice-versa. Delete DF row perhaps?\n",
    "    for index, row in df.iterrows():\n",
    "    \n",
    "        if row['time_dif'] < time_gap:\n",
    "            n = n + 1b\n",
    "            \n",
    "            print('Ordering {}:'.format(row['is_name']))\n",
    "\n",
    "            date = str(row['is_date'].year) + '-' + str(row['is_date'].month) + '-' + str(row['is_date'].day)\n",
    "            \n",
    "            #create directory for output\n",
    "            if os.path.isdir(directory_path + '/' + date) == False:\n",
    "                os.mkdir(directory_path + '/' + date)\n",
    "                \n",
    "            out_path = directory_path + '/' + date\n",
    "            print(date)\n",
    "            print(row['is_date'])\n",
    "            #Download the track\n",
    "            try:\n",
    "                tracks = ipx.Query(p_name, row['polygon'], date_range=[date, date])\n",
    "                tracks.earthdata_login(earthdata_uid, email)\n",
    "                tracks.order_granules(email=False)\n",
    "            except:\n",
    "                print('protocol error')\n",
    "            try:\n",
    "                tracks.download_granules(out_path)\n",
    "            except:\n",
    "                print('http error, file not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the Sentinel-1/2 Images\n",
    "\n",
    "This works for both Sentinel-1 and Sentinel-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_sentinel_data(time_gap, df, directory_path, api):\n",
    "    #Check that the User wants to download the products\n",
    "    input('Press Enter to order & download the Sentinel products')\n",
    "    #for each track in dataframe, download if the time gap is lower than input\n",
    "    #Note: If the download fails, the Sentinel image will still be downloaded and vice-versa. \n",
    "    for index, row in df.iterrows():\n",
    "        if row['time_dif'] < time_gap:\n",
    "            date = str(row['is_date'].year) + '-' + str(row['is_date'].month) + '-' + str(row['is_date'].day)\n",
    "            out_path = directory_path + '/' + date\n",
    "            print('Ordering {}:'.format(row['s_name']))\n",
    "            api.download(row['s_id'], directory_path=out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting IS2 H5 to Shapefile\n",
    "\n",
    "For Future: Plot the Height/Freeboard and save as png during this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For reading ATL10\n",
    "def getATL10(f, beam):\n",
    "    print('reading atl10')\n",
    "    lons = f[beam + '/freeboard_beam_segment/beam_freeboard/longitude'][:]\n",
    "    lats = f[beam + '/freeboard_beam_segment/beam_freeboard/latitude'][:]\n",
    "    freeboard = f[beam + '/freeboard_beam_segment/beam_freeboard/beam_fb_height'][:]\n",
    "    ssh_flag = f[beam + '/freeboard_beam_segment/height_segments/height_segment_ssh_flag'][:]\n",
    "    stype = f[beam + '/freeboard_beam_segment/height_segments/height_segment_type'][:]\n",
    "    \n",
    "    df10 = pd.DataFrame({'lats':lats, 'lons':lons, 'freeboard':freeboard, 'stype':stype, 'ssh_flag':ssh_flag})\n",
    "    return df10\n",
    "\n",
    "#Main Function\n",
    "def h5_to_shp(h5_file, out_path, name, p_name):\n",
    "    #integrate into download\n",
    "    \n",
    "    f = h5py.File(h5_file, 'r')\n",
    "    #Read columns based on product\n",
    "    if p_name == 'ATL07':\n",
    "        #Check which orientation is the strong beam\n",
    "        if ['orbit_info/sc_orient'][0] == 0:\n",
    "            df_1 = rd.getATL07(f, 'gt1l')\n",
    "            df_2 = rd.getATL07(f, 'gt2l')\n",
    "            df_3 = rd.getATL07(f, 'gt3l')\n",
    "        else:\n",
    "            df_1 = rd.getATL07(f, 'gt1r')\n",
    "            df_2 = rd.getATL07(f, 'gt2r')\n",
    "            df_3 = rd.getATL07(f, 'gt3r')\n",
    "    elif p_name == 'ATL06':\n",
    "        if ['orbit_info/sc_orient'][0] == 0:\n",
    "            df_1 = rd.getATL06(f, 'gt1l')\n",
    "            df_2 = rd.getATL06(f, 'gt2l')\n",
    "            df_3 = rd.getATL06(f, 'gt3l')\n",
    "        else:\n",
    "            df_1 = rd.getATL06(f, 'gt1r')\n",
    "            df_2 = rd.getATL06(f, 'gt2r')\n",
    "            df_3 = rd.getATL06(f, 'gt3r')\n",
    "        \n",
    "    elif p_name == 'ATL03':\n",
    "        if ['orbit_info/sc_orient'][0] == 0:\n",
    "            df_1 = rd.getATL03(f, 'gt1l')\n",
    "            df_2 = rd.getATL03(f, 'gt2l')\n",
    "            df_3 = rd.getATL03(f, 'gt3l')\n",
    "        else:\n",
    "            df_1 = rd.getATL03(f, 'gt1r')\n",
    "            df_2 = rd.getATL03(f, 'gt2r')\n",
    "            df_3 = rd.getATL03(f, 'gt3r')\n",
    "    \n",
    "    elif p_name == 'ATL10':\n",
    "        if ['orbit_info/sc_orient'][0] == 0:\n",
    "            df_1 = getATL10(f, 'gt1l')\n",
    "            df_2 = getATL10(f, 'gt2l')\n",
    "            df_3 = getATL10(f, 'gt3l')\n",
    "        else:\n",
    "            df_1 = getATL10(f, 'gt1r')\n",
    "            df_2 = getATL10(f, 'gt2r')\n",
    "            df_3 = getATL10(f, 'gt3r')\n",
    "            \n",
    "    #Get coordinates of each point and convert to Shapely Point         \n",
    "    geometry_1 = [Point(xy) for xy in zip(df_1.lons,df_1.lats)]\n",
    "    geometry_2 = [Point(xy) for xy in zip(df_2.lons,df_2.lats)]\n",
    "    geometry_3 = [Point(xy) for xy in zip(df_3.lons,df_3.lats)]\n",
    "    \n",
    "    #Load into GeoDataFrame\n",
    "    gdf_1 = gpd.GeoDataFrame(df_1, crs=\"EPSG:4326\", geometry=geometry_1)\n",
    "    gdf_2 = gpd.GeoDataFrame(df_2, crs=\"EPSG:4326\", geometry=geometry_2)\n",
    "    gdf_3 = gpd.GeoDataFrame(df_3, crs=\"EPSG:4326\", geometry=geometry_3)\n",
    "    \n",
    "    \n",
    "    #Convert to Shapefile\n",
    "    print('Writing Left beam....')\n",
    "    gdf_1.to_file(out_path + '/' + name +  '_1.shp')\n",
    "    print('Writing Centre beam....')\n",
    "    gdf_2.to_file(out_path + '/' + name + '_2.shp')\n",
    "    print('Writing Right beam....')\n",
    "    gdf_3.to_file(out_path + '/' + name + '_3.shp')\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract tiff from Sentinel-1 Zip\n",
    "\n",
    "Extracts all Polarizations (Usually HH and HV)\n",
    "Only works for GRD products (not SLC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_S1_tiff(zip_file, destination):\n",
    "    with ZipFile(zip_file, 'r') as zipObj:\n",
    "        file_list = zipObj.namelist()\n",
    "        for filename in file_list:\n",
    "            if filename.endswith('.tiff') or filename.endswith('.tiff'):\n",
    "                print('Extracting ' + filename)\n",
    "                zipObj.extract(filename, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining these functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all Sentinel images\n",
    "if platform = 'Sentinel-1':\n",
    "    pid_list = si.find_S1_images(footprint, date_1, date_2, api)\n",
    "elif platform = 'Sentinel-2':\n",
    "    pid_list = si.find_S1_images(footprint, date_1, date_2, api, max_cloud)\n",
    "print('%i Images found'%len(pid_list))\n",
    "\n",
    "#DataFrame to store tracks & images\n",
    "df = pd.DataFrame(columns=['is_name', 'is_date', 's_name', 's_date', 'time_dif', 's_id', 'polygon'])\n",
    "\n",
    "#For each Sentinel images, search for ICESat-2 tracks \n",
    "for pid in pid_list:\n",
    "    try:\n",
    "        si.find_IS_tracks(pid, p_name, df, api, is2_footprint) #footprint optional argument to restrict is_2 to section of sentinel image\n",
    "        print('Finding tracks for {}'.format(pid) )\n",
    "    except AssertionError:\n",
    "        print('No tracks found')\n",
    "\n",
    "#Sort & Display all coincident tracks found to the User along with their time separation        \n",
    "df=df.sort_values(by='time_dif', ascending=True)\n",
    "df=df.drop_duplicates(subset='is_name',keep='first')\n",
    "for index, row in df.iterrows():\n",
    "    print(str(row['time_dif']) + ' | ' +  row['is_name'] + ' | ' + row['s_name'])\n",
    "\n",
    "#Let the user decide how many hours the time window should be\n",
    "#Note This could be hard coded, but leaving it allows the user to decide how many images to download\n",
    "input_time = float(input('Enter time window for download in hours:'))\n",
    "time_gap = timedelta(hours=input_time)\n",
    "\n",
    "#Get time to create a folder to store output. This prevents the creation of duplicate folders\n",
    "now_time = datetime.now()\n",
    "now = now_time.strftime('%d%m%Y_%H%M%S')\n",
    "os.mkdir(out_path + 'IS2_' + platform + '_'  + now)\n",
    "dest_folder = out_path + 'IS2_' + platform + '_'  + now\n",
    "\n",
    "#Create a CSV to keep record of the coicident files\n",
    "df.to_csv(dest_folder + '/' + 'output.csv', sep=',')\n",
    "\n",
    "#Download the IS2 data\n",
    "#Note: At present, the user is prompted to enter the password for each individual download\n",
    "#There must be a way to avoid this but not sure how. \n",
    "si.dl_is2_data(time_gap, df, p_name, dest_folder, earthdata_uid, email)\n",
    "\n",
    "#Iterate through the output folders and convert them all to shapefiles\n",
    "for root,dirs,files in os.walk(dest_folder, topdown=False):\n",
    "    for name in files:\n",
    "       f_path = os.path.join(root, name)\n",
    "       if f_path.endswith('.h5'):\n",
    "           print('Converting ' + name + ' to Shapefile')\n",
    "           out_path = root + '/georef_tracks'\n",
    "           if os.path.isdir(out_path) == False:\n",
    "               os.mkdir(out_path)\n",
    "           try:\n",
    "               si.h5_to_shp(f_path, out_path, name, p_name)\n",
    "           except KeyError:\n",
    "               print('KeyError')\n",
    "           \n",
    "           \n",
    "#Download the Sentinel data\n",
    "dl_sentinel_data(time_gap, df, dest_folder, api)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    " # Finding Sentinel 1/2 Images for given Icesat-2 Tracks\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this setup, the user enters a directory full of ICESat-2 hdf5 files (is_path). The script extracts the time and location of the track and searches for nearby Sentinel Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Coordinates from IS-2 track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_is_footprint(track,pname):        \n",
    "    f = h5py.File(track, 'r')\n",
    "    \n",
    "    #Read product\n",
    "    if pname == 'atl10':\n",
    "        if f['orbit_info/sc_orient'][0] == 0:\n",
    "            print('left')\n",
    "            df_1 = rd.getATL10data(track, 'gt1l')\n",
    "        else:\n",
    "            print('right')\n",
    "            df_1 = rd.getATL10data(track, 'gt1r')         \n",
    "    \n",
    "    if pname == 'atl07':\n",
    "        if f['orbit_info/sc_orient'][0] == 0:\n",
    "            print('left')\n",
    "            df_1 = rd.getATL07(f, 'gt1l')            \n",
    "        else:\n",
    "            print('right')\n",
    "            df_1 = rd.getATL07(f, 'gt1r')\n",
    "    \n",
    "        \n",
    "    df_len = df_1.shape[0]\n",
    "    print(df_len)\n",
    "    \n",
    "    #Get Coorodinates of points along track\n",
    "    #This is not a good solution as it could miss images or find the same image twice, but is the only way I could get ..\n",
    "    #it working consistently\n",
    "    \n",
    "    lat1, lon1 = df_1.lats.iloc[0], df_1.lons.iloc[0]\n",
    "    lat2, lon2 = df_1.lats.iloc[round(df_len/4)], df_1.lons.iloc[round(df_len/4)]\n",
    "    lat3, lon3 = df_1.lats.iloc[round(df_len/2)], df_1.lons.iloc[round(df_len/2)]\n",
    "    lat4, lon4 = df_1.lats.iloc[round(df_len*0.75)], df_1.lons.iloc[round(df_len*0.75)]\n",
    "    lat5, lon5 = df_1.lats.iloc[-1], df_1.lons.iloc[-1]\n",
    "    \n",
    "    footprint_1 = 'POINT('+str(lon1)+' '+str(lat1)+')'\n",
    "    footprint_2 = 'POINT('+str(lon2)+' '+str(lat2)+')'\n",
    "    footprint_3 = 'POINT('+str(lon3)+' '+str(lat3)+')'\n",
    "    footprint_4 = 'POINT('+str(lon4)+' '+str(lat4)+')'\n",
    "    footprint_5 = 'POINT('+str(lon5)+' '+str(lat5)+')'\n",
    "    \n",
    "    footprint_list = [footprint_1, footprint_2, footprint_3, footprint_4, footprint_5]\n",
    "    print('got here')\n",
    "    \n",
    "    return footprint_list  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Finding and Downloading Sentinel-1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentinel1(footprint_list, date_1, date_2, api, out_path):\n",
    "    global pid_list\n",
    "    pid_list = []\n",
    "    for n in footprint_list:\n",
    "        print(n)\n",
    "        products = api.query(n, \n",
    "                             date=(date_1, date_2), \n",
    "                             platformname = 'Sentinel-1', \n",
    "                             sensoroperationalmode = 'EW',\n",
    "                             producttype='GRD',\n",
    "                             area_relation='Intersects')\n",
    "        if api.get_products_size(products) != 0:\n",
    "            print(products)\n",
    "            api.download_all(products, directory_path=out_path)   \n",
    "    return pid_list\n",
    "\n",
    "def find_sentinel2(footprint_list, date_1, date_2, api, out_path, max_cloud):\n",
    "    \n",
    "    global pid_list\n",
    "    pid_list = []  \n",
    "    for n in footprint_list:\n",
    "        products = api.query(n, \n",
    "                             date=(date_1, date_2), \n",
    "                             platformname = 'Sentinel-2', \n",
    "                             cloudcoverpercentage=(0, max_cloud),\n",
    "                             area_relation='Intersects',\n",
    "                             producttype='S2MSI1C')\n",
    "\n",
    "        if api.get_products_size(products) != 0:\n",
    "            print(products)\n",
    "            api.download_all(products, directory_path=out_path)\n",
    "    return pid_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining all three functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "is_path = 'E:/Cryo2ice/cryo2ice_august/'\n",
    "api = SentinelAPI('username', 'password', 'https://scihub.copernicus.eu/dhus')\n",
    "pname = 'atl07' #This should maybe be identified from filename to support different porducts\n",
    "    \n",
    "#iterate through all is2 files\n",
    "for i in os.listdir(is_path):\n",
    "    if i.endswith('.h5'):\n",
    "        track = is_path + i\n",
    "        #extract date\n",
    "        ac_time = i[9:23]\n",
    "        is_time = datetime.datetime.strptime(ac_time, '%Y%m%d%H%M%S')\n",
    "        two_hours = datetime.timedelta(hours=6)#Changes hours to limit sea ice drift\n",
    "        before_time = is_time - two_hours\n",
    "        after_time = is_time + two_hours\n",
    "        footprint_list = get_is_footprint(track,pname) #Get IS2 coordinates\n",
    "        out_path = is_path + ac_time + '/'\n",
    "        os.mkdir(out_path)  #Put output into new file and copy track into\n",
    "        copyfile(track, out_path + i)\n",
    "        #Find Sentinel1/2 change function to whichever is wanted. If Sentinel-2, max_cloud is needed\n",
    "        \n",
    "        find_sentinel2(footprint_list, before_time, after_time, api, out_path, max_cloud=60)\n",
    "        print('Finding Sentinel images')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
